#!/usr/bin/env python

###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Joel Boyd, Ben Woodcroft, Donovan Parks"
__copyright__ = "Copyright 2015"
__credits__ = ["Joel Boyd", "Ben Woodcroft", "Donovan Parks"]
__license__ = "GPL3"
__maintainer__ = "Joel Boyd, Ben Woodcroft, Donovan Parks"
__email__ = "donovan.parks@gmail.com"
__status__ = "Development"

import argparse
import logging
import sys
import os
import subprocess
import IPython
import shutil

from mingle.custom_help_formatter import CustomHelpFormatter
from mingle.graftm_stockholm_io import GraftMStockholmIterator
from mingle.arb_parser import ArbParser
from mingle.reduce_workflow import Reduce
from mingle.bootstrap import Bootstrap


def version():
    """Read program version from file."""
    bin_dir = os.path.dirname(os.path.realpath(__file__))
    version_file = open(os.path.join(bin_dir, '..', 'mingle', 'VERSION'))
    return version_file.read().strip()


def printHelp():
    print ''
    print '                ...::: mingle v' + version() + ' :::...'''
    print '''\

  Gene tree inference:
    hmm        -> Infer gene tree using a HMM
    blast      -> Infer gene tree using BLAST

  Tree utilities:
    reduce     -> Infer tree for reduced set of genes
    bootstrap  -> Calculate bootstrap support for tree
    '''


# Given an iterable (e.g. a list) of taxonomies, write a new file containing a taxtastic-compatible
# taxonomy file to the given (presumed open) file handle. The ranks is a list of
# e.g. ['phylum','class']


# Return a hash of sequence name => sequence from the given stockholm
# file. Only return the sequence that is aligned to the HMM (not the lower
# case inserts)
def aligned_sequences_from_sto_file(sto_file):
    return GraftMStockholmIterator().readAlignedSequences(open(sto_file))


# Correct known instances of dodgy Archaea taxonomy. Desparate times, desparate measures.
def check_taxonomy(tax, ace_id_to_taxonomy):
    tax_suffix = ['d__', 'k__', 'p__', 'c__', 'o__', 'f__', 'g__', 's__']
    for _ace_id, _taxes in ace_id_to_taxonomy.items():
        tax_set = tax.split('; ')

        if 'k__Crenarchaeota' in tax_set:

            if tax_set[2] == 'p__':
                del tax_set[-1]
                for idx, item in enumerate(tax_set):
                    tax_set[idx] = item.replace(tax_suffix[idx], tax_suffix[idx + 1])
                return '; '.join(tax_set)
            elif tax_set[2] == 'p__Thaumarchaeota':
                del tax_set[4]
                for idx, item in enumerate(tax_set):
                    tax_set[idx] = item.replace(item[:3], tax_suffix[idx + 1])
                return '; '.join(tax_set)

            elif tax_set[2] == 'p__Crenarchaeota':
                del tax_set[3]
                for idx, item in enumerate(tax_set):
                    tax_set[idx] = item.replace(item[:3], tax_suffix[idx + 1])
                return '; '.join(tax_set)

        elif 'k__Euryarchaeota' in tax_set:
            del tax_set[1]
            for idx, item in enumerate(tax_set):
                tax_set[idx] = item.replace(item[:3], tax_suffix[idx + 1])
            return '; '.join(tax_set)
        else:
            del tax_set[-1]
            for idx, item in enumerate(tax_set):
                tax_set[idx] = item.replace(tax_suffix[idx], tax_suffix[idx + 1])
            return '; '.join(tax_set)


def hmm_workflow(options):
    """Infer gene tree using HMM workflow."""

    from mingle.getaxnseq import Tax_n_Seq_builder
    from mingle.taxonomy_string import TaxonomyString
    from mingle.rerooter import Rerooter

    # Read Phil format file, creating a hash of ACE ID to taxonomy
    logging.info('Reading taxonomy information from ARB GreenGenes file.')
    ace_id_to_taxonomy = {}
    ace_id_to_greengenes_hash = {}
    ids_used = []
    for entry in ArbParser().each(open(options.greengenes)):
        ace_id = entry['db_name']
        if options.public:
            if entry['core_list_status'] == 'public'or entry['prokMSA_id'][0] == 'A':
                ids_used.append(entry['db_name'])
                try:
                    taxonomy = entry['genome_tree_tax_string']
                    ace_id_to_taxonomy[ace_id] = TaxonomyString(taxonomy).full_name()
                    ace_id_to_greengenes_hash[ace_id] = entry
                except KeyError:
                    logging.warn("taxonomy information not found in Phil format file for ID: %s, skipping" % ace_id)
        if not options.public:
            ids_used.append(entry['db_name'])
            try:
                taxonomy = entry['genome_tree_tax_string']
                ace_id_to_taxonomy[ace_id] = TaxonomyString(taxonomy).full_name()
                ace_id_to_greengenes_hash[ace_id] = entry
            except KeyError:
                logging.warn("taxonomy information not found in Phil format file for ID: %s, skipping" % ace_id)

    logging.info("Read in taxonomy for %s genomes" % len(ace_id_to_taxonomy))

    logging.info('Creating simple taxonomy format file..')
    simple_taxonomy_file_path = os.path.join(options.output_dir, 'taxonomy.csv')
    with open(simple_taxonomy_file_path, 'w') as taxonomy_fh:

        for ace_id, _taxes in ace_id_to_taxonomy.items():
            tax = ace_id_to_greengenes_hash[ace_id]['genome_tree_tax_string']

            if tax:
                if tax.endswith(';'):
                    tax = tax[:-1]

                if TaxonomyString(tax).num_levels() == 7:
                    taxonomy_fh.write(ace_id)
                    taxonomy_fh.write("\t")
                    taxonomy_fh.write(tax)  # Need the full path including empty names for tax2tree
                    taxonomy_fh.write("\n")
                else:
                    logging.warn("Found unexpected number of taxonomy levels in this taxonomy string, not outputing this taxonomy: %s. Attempting to correct..." % tax)
                    tax2 = check_taxonomy(tax, ace_id_to_taxonomy)

                    taxonomy_fh.write(ace_id)
                    taxonomy_fh.write("\t")
                    taxonomy_fh.write(tax2)  # Need the full path including empty names for tax2tree

                    taxonomy_fh.write("\n")

    logging.info('taxonomy file created.')

    logging.info('Create taxit taxonomy and seqinfo file.')

    taxtastic_tax = os.path.join(options.output_dir, 'tax_info.csv')
    taxtastic_seq = os.path.join(options.output_dir, 'seq_info.csv')
    Tax_n_Seq_builder().gg_taxonomy_builder(simple_taxonomy_file_path, taxtastic_tax, taxtastic_seq)
    logging.info('taxonomy and seqinfo file created.')

    u_proteome_files = os.path.join(options.output_dir, 'used_proteome_files.txt')
    ids_used = set(ids_used)
    ids_files = []
    for f in open(options.proteomes_list_file, 'r'):
        if any(ids in f for ids in ids_used):
            ids_files.append(f)
    with open(u_proteome_files, 'w') as prot_files:
        for i in ids_files:
            prot_files.write(i)

    # Read each of the proteome paths and strip() them
    with open(u_proteome_files) as f:
        proteomes = [pro.strip() for pro in f]

    if len(proteomes) < 1:
        raise Exception("Error: no proteome files found, cannot continue")
    logging.info("Read in %s proteome files for processing e.g. %s" % (len(proteomes), proteomes[0]))

    # Run hmmsearch on each of the proteomes
    logging.info("Running hmmsearches..")
    hmmsearches_directory = os.path.join(options.output_dir, 'hmmsearches')
    os.makedirs(hmmsearches_directory)
    # TODO: the below fails if the given proteome list file is a pipe as opposed to a regular file. This script should read the list file,
    # and then directly pass the list to parallel, and then all will be well.
    command = "cat %s |parallel --gnu -j %s hmmsearch -E %g --domtblout %s/{/}.domtblout.csv %s {} '>' /dev/null" % (
      u_proteome_files,
      options.cpus,
      options.evalue,
      hmmsearches_directory,
      options.hmm)
    logging.info("Running cmd: %s" % command)
    subprocess.check_call(["/bin/bash", "-c", command])

    # Extract the proteins from each proteome that were hits
    # hmmalign the hits back to the HMM
    logging.info("Extracting hit proteins from proteome files and aligning hit proteins back to HMM..")
    aligned_proteins_directory = os.path.join(options.output_dir, 'hmmaligns')
    os.makedirs(aligned_proteins_directory)

    # hmmalign ../nifH.HMM <(fxtract -H -f <(grep -v ^\# ../hmmsearches/C00001470.fna.faa.domtblout.csv |awk '{print $1}' |sort |uniq) ../proteomes/C00001470.fna.faa) |seqmagick convert --input-format stockholm --output-format fasta - -
    # grep -v ^\# C00001470.fna.faa.domtblout.csv |awk '{print $1}' |sort |uniq |fxtract -H -f /dev/stdin ../proteomes/C00001470.fna.faa
    align_and_extract_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'alignAsNecessary.py')
    # Usage: alignAsNecessary.py fasta_file hmm output_stockholm

    command = "cat %s |parallel --gnu -j %s grep -v ^\# %s/{/}.domtblout.csv '|' cut -d'\" \"' -f1 '|' sort -u '|' %s {} %s %s/{/}.sto" % (
      u_proteome_files,
      options.cpus,
      hmmsearches_directory,
      align_and_extract_script,
      options.hmm,
      aligned_proteins_directory,
      )
    logging.info("Running cmd: %s" % command)
    subprocess.check_call(["/bin/bash", "-c", command])

    # Remove the unaligned parts of the stockholm file
    logging.info("Concatenating and renaming files into a single aligned fasta file and creating Phil format output file..")
    # => use biopython's sto parser
    # For each of the hmmaligned files, read in the
    sequence_number = 0
    greengenes_output_hashes = []

    aligned_sequences_file = os.path.join(options.output_dir, 'aligned.fasta')
    duplicates = {}

    with open(aligned_sequences_file, 'w') as aligned_sequences_fh:
        for proteome in proteomes:
            # Skip if the file does not exist - this indicates there was no hits
            sto_path = os.path.join(aligned_proteins_directory, "%s.sto" % os.path.basename(proteome))
            if not os.path.exists(sto_path):
                continue

            db_id = os.path.basename(proteome).split('.')[0]
            seqs = aligned_sequences_from_sto_file(sto_path)
            n_seqs = len(seqs.keys())
            s = [seqs[i] for i in seqs.keys()]

            try:
                tax = ace_id_to_taxonomy[db_id]
                key_values = ace_id_to_greengenes_hash[db_id]
            except KeyError:
                tax = None
                key_values = None

            if n_seqs > 1:
                logging.info("%s has multiple sequences. Renaming..." % (db_id))
                names = [db_id + 'n' + str(x) for x in range(0, n_seqs)]
                duplicates[db_id] = names
            else:
                names = [db_id]

            for name, seq in zip(names, s):
                aligned_sequences_fh.write(">%s\n" % name)
                aligned_sequences_fh.write("%s\n" % seq)
                sequence_number += 1

                # Writing a mingle readable file
                output_description_hash = {}
                output_description_hash['db_name'] = name
                output_description_hash['prokMSA_id'] = name
                output_description_hash['name'] = name
                output_description_hash['acc'] = name
                output_description_hash['aligned_seq'] = seq

                if tax is not None:
                    direct_copy_fields = [
                                        'organism',
                                        'genome_tree_description',
                                        # 'prokMSA_id',
                                        'owner',
                                        'genome_tree_tax_string',
                                        'greengenes_tax_string',
                                        'blast_hits_16s',
                                        'img_tax_string',
                                        'img_tax_tax2tree_corrected',
                                        'checkm_completeness',
                                        'checkm_contamination',
                                        'core_list_status',
                                        'warning',
                                            ]

                    for field in direct_copy_fields:
                        try:
                            output_description_hash[field] = key_values[field]
                        except KeyError:
                            pass  # if it ain't there, don't include it

                greengenes_output_hashes.append(output_description_hash)

    # Actually write phil readable file
    logging.info("Writing output Phil format file..")
    phil_format_output_file = os.path.join(options.output_dir, 'mingle.greengenes')

    with open(phil_format_output_file, 'w') as f:
        ArbParser().write(greengenes_output_hashes, f)

    logging.info("Correcting the sequence names of duplicates in the seq_info file.")
    new_seq = []
    with open(taxtastic_seq, 'r') as f:
        ids = [i.strip().split(',') for i in f]

        for i in ids:
            try:
                for new in duplicates[i[0]]:
                    new_seq = new_seq + [','.join([new, i[1]])]
            except:
                new_seq = new_seq + [','.join(i)]

    with open(taxtastic_seq, 'w') as f:
        for entry in new_seq:
            f.write("%s\n" % entry)

    logging.info("Included %s sequences in the alignment" % (sequence_number))
    if sequence_number == 0:
        raise Exception("No matching sequences detected, cannot create tree")
    if sequence_number < 4:
        logging.warn("Too few sequences detected (%s)!!!!, the output is unlikely to be informative" % sequence_number)

    # Make a tree, saving the log information
    logging.info("Creating phylogenetic tree..")
    fasttree_output = os.path.join(options.output_dir, 'fasttree.tree')
    fasttree_log_file = os.path.join(options.output_dir, 'fasttree.log')
    subprocess.check_call(["bash", "-c", "FastTreeMP -log %s %s > %s" % (fasttree_log_file, aligned_sequences_file, fasttree_output)])

    # Reroot the tree
    logging.info("Rerooting phylogenetic tree..")
    rerooted_tree = os.path.join(options.output_dir, 'rerooted.tree')
    Rerooter().main(fasttree_output, rerooted_tree, options.nodes)

    # # TODO - remake log files
    logging.info("Creating log file for rerooted tree..")
    rerooted_log = os.path.join(options.output_dir, 'rerooted.log')
    rerooted2_tree = os.path.join(options.output_dir, 'rerooted2.tree')
    cmd = "FastTreeMP -nome -mllen -intree %s -log %s %s > %s" % (rerooted_tree, rerooted_log, aligned_sequences_file, rerooted2_tree)
    subprocess.check_call(["bash", "-c", cmd])

    # Create the final reference package
    logging.info("Creating reference package by taxit create")
    base = os.path.basename(options.hmm).split('.')[0]
    refpkg_output = '%s.refpkg' % (base)
    command = "taxit create -f %s -P %s -t %s -s %s -c -l %s -T %s -i %s --no-reroot" % (aligned_sequences_file, refpkg_output, rerooted2_tree, rerooted_log, base, taxtastic_tax, taxtastic_seq)
    subprocess.check_call(["/bin/bash", "-c", command])

    # Move houses
    logging.info("Compiling GPKG")
    gpkg_name = base + "_gpkg"
    os.mkdir(gpkg_name)
    shutil.move(options.hmm, gpkg_name)
    shutil.move(refpkg_output, gpkg_name)

    logging.info("Finished")


def blast_workflow(options):
    """Infer gene tree using BLAST workflow."""

    from mingle.blast_workflow import BlastWorkflow

    blast_workflow = BlastWorkflow(options.cpus)
    blast_workflow.run(options.query_proteins,
                       options.db_file,
                       options.taxonomy_file,
                       options.evalue,
                       options.per_identity,
                       options.per_aln_len,
                       options.max_matches,
                       options.blast_mode,
                       options.min_per_taxa,
                       options.min_per_bp,
                       options.restrict_taxon,
                       options.msa_program,
                       options.output_dir)

    sys.exit(0)  # break free from HMM workflow [SUPER HACKY]


if __name__ == '__main__':
    parser = argparse.ArgumentParser(add_help=False)
    subparsers = parser.add_subparsers(help="--", dest='subparser_name')

    # options for inferring a gene tree using a HMM
    # greengenes file from genome tree database => provides taxonomy
    # HMM => the HMM to search with
    # path to proteome files - one for each genome => sequences to be put into the tree
    default_evalue = 1e-20

    hmm_parser = subparsers.add_parser('hmm',
                                        add_help=False,
                                        formatter_class=CustomHelpFormatter,
                                        description='Infer gene tree using a HMM.')

    req_args = hmm_parser.add_argument_group('required arguments')
    req_args.add_argument('--hmm', help='Hidden Markov model to search with', required=True)
    req_args.add_argument('--greengenes', help='GreenGenes file with metadata for genomes', required=True)
    req_args.add_argument('--proteomes_list_file', help='path to file containing faa files of each genome\'s proteome (of the form [path]<ace_ID>[stuff].faa e.g. /srv/home/ben/A00000001.fna.faa', required=True)
    req_args.add_argument('--output_dir', help='store files in this folder', required=True)
    req_args.add_argument('--nodes', help='node names to re-root with', default=None, required=True)

    optonal_args = hmm_parser.add_argument_group('optional arguments')
    optonal_args.add_argument('--evalue', help='evalue cutoff for identifying homologs', type=float, default=default_evalue)
    optonal_args.add_argument('--cpus', type=int, help='number of CPUs to use throughout the process', default=1)
    optonal_args.add_argument('--public', action="store_true", help='Only search proteomes with core_list_status set to public', default=False)
    optonal_args.add_argument('-h', '--help', action="help", help="show help message")

    # options for inferring a gene tree using blast
    blast_parser = subparsers.add_parser('blast',
                                        add_help=False,
                                        formatter_class=CustomHelpFormatter,
                                        description='Infer gene tree using BLAST.')

    req_args = blast_parser.add_argument_group('required arguments')
    req_args.add_argument('-q', '--query_proteins', help='protein sequences for identifying homologs (fasta format)', required=True)
    req_args.add_argument('-d', '--db_file', help='BLAST database of reference proteins', required=True)
    req_args.add_argument('-t', '--taxonomy_file', help='taxonomic assignment of each reference genomes', required=True)
    req_args.add_argument('-o', '--output_dir', help='output directory', required=True)

    optonal_args = blast_parser.add_argument_group('optional arguments')
    optonal_args.add_argument('-e', '--evalue', help='evalue cutoff for identifying homologs', type=float, default=1e-5)
    optonal_args.add_argument('-p', '--per_identity', help="percent amino acid identity for identifying homologs", type=float, default=30.0)
    optonal_args.add_argument('-a', '--per_aln_len', help="percent alignment length of query sequence for identifying homologs", type=float, default=50.0)
    optonal_args.add_argument('-m', '--max_matches', help="maximum number of matches per query protein", type=int, default=500)
    optonal_args.add_argument('--blast_mode', choices=['blastp', 'blastp-fast'], help="type of BLASTP to perform", default='blastp-fast')
    optonal_args.add_argument('--min_per_taxa', help='minimum percentage of taxa required required to retain column', type=float, default=50.0)
    optonal_args.add_argument('--min_per_bp', help='minimum percentage of base pairs required to keep trimmed seqs', type=float, default=50.0)
    optonal_args.add_argument('--restrict_taxon', help='restrict alignment to specific taxonomic group (e.g., d__Archaea)', default=None)
    optonal_args.add_argument('--msa_program', choices=['mafft', 'muscle'], help="program to use for multiple sequence alignment", default='mafft')
    optonal_args.add_argument('--cpus', type=int, help='CPUs to use throughout the process', default=1)
    optonal_args.add_argument('-h', '--help', action="help", help="show help message")

    # infer tree over a reduced set of genes
    reduce_parser = subparsers.add_parser('reduce',
                                        add_help=False,
                                        formatter_class=CustomHelpFormatter,
                                        description='Infer tree for reduced set of genes.')

    req_args = reduce_parser.add_argument_group('required arguments')
    req_args.add_argument('-m', '--msa_file', help='multiple sequence alignment used to infer tree (fasta format)', required=True)
    req_args.add_argument('-g', '--gene_ids', help='gene ids to retain in tree (one id per line)', required=True)
    req_args.add_argument('-t', '--taxonomy_file', help='taxonomic assignment of genes in gene tree', required=True)
    req_args.add_argument('-o', '--output_tree', help='output tree', required=True)

    optonal_args = reduce_parser.add_argument_group('optional arguments')
    optonal_args.add_argument('--cpus', type=int, help='CPUs to use throughout the process', default=1)
    optonal_args.add_argument('-h', '--help', action="help", help="show help message")

    # calculate bootstrap support
    bootstrap_parser = subparsers.add_parser('bootstrap',
                                        add_help=False,
                                        formatter_class=CustomHelpFormatter,
                                        description='Calculate bootstrap support for tree.')

    req_args = bootstrap_parser.add_argument_group('required arguments')
    req_args.add_argument('-t', '--tree', help='tree requiring bootstrap support values', required=True)
    req_args.add_argument('-m', '--msa_file', help='multiple sequence alignment used to infer tree (fasta format)', required=True)
    req_args.add_argument('-o', '--output_tree', help='output tree with bootstrap values', required=True)

    optonal_args = bootstrap_parser.add_argument_group('optional arguments')
    optonal_args.add_argument('-r', '--num_replicates', help='number of bootstrap replicates to perform', type=int, default=100)
    optonal_args.add_argument('--cpus', type=int, help='CPUs to use throughout the process', default=1)
    optonal_args.add_argument('-h', '--help', action="help", help="show help message")

    # get and check options
    options = None
    if(len(sys.argv) == 1 or sys.argv[1] == '-h' or sys.argv == '--help'):
        printHelp()
        sys.exit(0)
    else:
        options = parser.parse_args()

    # setup general properties of logger
    logger = logging.getLogger('')
    logger.setLevel(logging.DEBUG)
    log_format = logging.Formatter(fmt="[%(asctime)s] %(levelname)s: %(message)s",
                                  datefmt="%Y-%m-%d %H:%M:%S")

    # setup logging to console
    stream_logger = logging.StreamHandler(sys.stdout)
    stream_logger.setFormatter(log_format)
    stream_logger.setLevel(logging.DEBUG)
    logger.addHandler(stream_logger)

    if options.subparser_name == 'reduce':
        r = Reduce(options.cpus)
        r.run(options.msa_file, options.gene_ids, options.taxonomy_file, options.output_tree)
        sys.exit(0)
    elif options.subparser_name == 'bootstrap':
        bootstrap = Bootstrap(options.cpus)
        bootstrap.run(options.tree, options.msa_file, options.output_tree, options.num_replicates)
        sys.exit(0)

    # create output directory
    if not os.path.exists(options.output_dir):
        os.mkdir(options.output_dir)

    # set file logging
    file_logger = logging.FileHandler(os.path.join(options.output_dir, 'mingle.log'), 'w')
    file_logger.setFormatter(log_format)
    logger.addHandler(file_logger)

    logger.info('Running mingle v%s' % version())
    logger.info(' '.join(sys.argv))

    if options.subparser_name == "hmm":
        hmm_workflow(options)
    elif options.subparser_name == 'blast':
        blast_workflow(options)
